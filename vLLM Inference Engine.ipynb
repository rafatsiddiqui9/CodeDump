{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q -U ipywidgets --no-cache-dir\n# !pip install -q vllm","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:06.757250Z","iopub.execute_input":"2024-07-11T22:27:06.758142Z","iopub.status.idle":"2024-07-11T22:27:06.764145Z","shell.execute_reply.started":"2024-07-11T22:27:06.758100Z","shell.execute_reply":"2024-07-11T22:27:06.763105Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!curl ipv4.icanhazip.com","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:06.769908Z","iopub.execute_input":"2024-07-11T22:27:06.770246Z","iopub.status.idle":"2024-07-11T22:27:07.844467Z","shell.execute_reply.started":"2024-07-11T22:27:06.770208Z","shell.execute_reply":"2024-07-11T22:27:07.843350Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"34.27.60.162\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:07.846379Z","iopub.execute_input":"2024-07-11T22:27:07.846649Z","iopub.status.idle":"2024-07-11T22:27:08.941901Z","shell.execute_reply.started":"2024-07-11T22:27:07.846624Z","shell.execute_reply":"2024-07-11T22:27:08.941003Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Thu Jul 11 22:27:08 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   60C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:08.943249Z","iopub.execute_input":"2024-07-11T22:27:08.943552Z","iopub.status.idle":"2024-07-11T22:27:08.960265Z","shell.execute_reply.started":"2024-07-11T22:27:08.943524Z","shell.execute_reply":"2024-07-11T22:27:08.959310Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'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'"},"metadata":{}}]},{"cell_type":"code","source":"!huggingface-cli login --add-to-git-credential --token  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","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:08.962618Z","iopub.execute_input":"2024-07-11T22:27:08.962911Z","iopub.status.idle":"2024-07-11T22:27:11.855366Z","shell.execute_reply.started":"2024-07-11T22:27:08.962887Z","shell.execute_reply":"2024-07-11T22:27:11.854439Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.device_count())","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:11.856627Z","iopub.execute_input":"2024-07-11T22:27:11.856906Z","iopub.status.idle":"2024-07-11T22:27:13.426471Z","shell.execute_reply.started":"2024-07-11T22:27:11.856881Z","shell.execute_reply":"2024-07-11T22:27:13.425503Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"break","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:13.427662Z","iopub.execute_input":"2024-07-11T22:27:13.428040Z","iopub.status.idle":"2024-07-11T22:27:13.434249Z","shell.execute_reply.started":"2024-07-11T22:27:13.428014Z","shell.execute_reply":"2024-07-11T22:27:13.433099Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"],"ename":"SyntaxError","evalue":"'break' outside loop (668683560.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport multiprocessing\nfrom vllm import LLM, SamplingParams\n\n# Set the multiprocessing start method to 'spawn'\nmultiprocessing.set_start_method('spawn', force=True)\n\nprompts = [\n    \"Hello, my name is\",\n    \"The president of the United States is\",\n    \"The capital of France is\",\n    \"The future of AI is\",\n]\n\n# Configure sampling parameters\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n# Initialize the LLM with optimized settings\nmodel = LLM(\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    dtype=torch.float16,  # Use half precision\n    tensor_parallel_size=2,  # Use a single GPU for now\n    gpu_memory_utilization=0.95,  # Use 95% of available GPU memory\n    max_num_batched_tokens=8192,  # Adjust based on your RAM\n)\n\n# Generate outputs\noutputs = model.generate(prompts, sampling_params)\n\n# Print the outputs\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T22:27:44.392170Z","iopub.execute_input":"2024-07-11T22:27:44.392817Z","iopub.status.idle":"2024-07-11T22:29:53.647998Z","shell.execute_reply.started":"2024-07-11T22:27:44.392772Z","shell.execute_reply":"2024-07-11T22:29:53.646826Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"WARNING 07-11 22:27:45 config.py:1354] Casting torch.bfloat16 to torch.float16.\nINFO 07-11 22:27:45 config.py:698] Defaulting to use mp for distributed inference\nINFO 07-11 22:27:45 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n","output_type":"stream"},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"INFO 07-11 22:27:46 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-11 22:27:46 selector.py:53] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:46 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:46 selector.py:53] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:47 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 07-11 22:27:48 utils.py:741] Found nccl from library libnccl.so.2\nINFO 07-11 22:27:48 pynccl.py:63] vLLM is using nccl==2.20.5\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:48 utils.py:741] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:48 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 07-11 22:27:48 custom_all_reduce_utils.py:202] generating GPU P2P access cache in /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 07-11 22:27:55 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m WARNING 07-11 22:27:55 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 07-11 22:27:55 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m WARNING 07-11 22:27:55 custom_all_reduce.py:127] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 07-11 22:27:56 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 07-11 22:27:56 selector.py:53] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:56 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:56 selector.py:53] Using XFormers backend.\nINFO 07-11 22:27:56 weight_utils.py:218] Using model weights format ['*.safetensors']\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:27:56 weight_utils.py:218] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72307c2f79e249e085f586167b3c0dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbeda80c69644e88f35680b1de56259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4602dae38e4caba8a65e646c826d46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220439dd19c9452d9491a03933159e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd6a0ae77b14c6593650f53ef458ca9"}},"metadata":{}},{"name":"stdout","text":"INFO 07-11 22:29:24 model_runner.py:255] Loading model weights took 7.4829 GB\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:29:24 model_runner.py:255] Loading model weights took 7.4829 GB\nINFO 07-11 22:29:28 distributed_gpu_executor.py:56] # GPU blocks: 4388, # CPU blocks: 4096\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:29:31 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:29:31 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 07-11 22:29:31 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 07-11 22:29:31 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=597)\u001b[0;0m INFO 07-11 22:29:52 model_runner.py:1117] Graph capturing finished in 21 secs.\nINFO 07-11 22:29:52 model_runner.py:1117] Graph capturing finished in 21 secs.\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 4/4 [00:00<00:00,  4.71it/s, est. speed input: 30.62 toks/s, output: 75.38 toks/s]","output_type":"stream"},{"name":"stdout","text":"Prompt: 'Hello, my name is', Generated text: \" Emma, and I'm a senior at the University of Iowa. I'm a\"\nPrompt: 'The president of the United States is', Generated text: ' the highest-ranking official in the executive branch of the federal government. The president is'\nPrompt: 'The capital of France is', Generated text: ' Paris. Paris is a city located in the north-central part of the country,'\nPrompt: 'The future of AI is', Generated text: \" here. And it's not just about chatbots.\\nAI is transforming industries and\"\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}