{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q -U ipywidgets --no-cache-dir\n# !pip install vllm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl ipv4.icanhazip.com","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom dotenv import load_dotenv\n\nos.environ['hf_token'] = \"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\"\nos.getenv('hf_token')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install vllm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom accelerate import Accelerator\n\n# Initialize the Accelerator\naccelerator = Accelerator()\n\n# Load the model and tokenizer with the token\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=os.getenv('hf_token'))\nmodel = AutoModelForCausalLM.from_pretrained(model_name,use_auth_token=os.getenv('hf_token'))\n\n# Move model to the appropriate device\nmodel = accelerator.prepare(model)\n\n# Define prompts\nprompts = [\n    \"Hello, my name is\"\n]\n\n# Tokenize prompts\ninputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n\n# Move inputs to the appropriate device\ninputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n\n# Generate outputs\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_length=50, temperature=0.8, top_p=0.95)\n\n# Decode and print outputs\ngenerated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nfor prompt, generated_text in zip(prompts, generated_texts):\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}